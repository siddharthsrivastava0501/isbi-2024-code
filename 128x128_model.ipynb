{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from math import floor\n",
    "import datetime\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2 as cv\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5786a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_voronoi_sites(input_img):\n",
    "    sites = []\n",
    "    \n",
    "    uintimage = input_img.copy()\n",
    "\n",
    "    # Flip the colours of the image\n",
    "    uintimage[uintimage == 255] = 2\n",
    "    uintimage[uintimage == 0] = 255\n",
    "    uintimage[uintimage == 2] = 0\n",
    "    \n",
    "    # Find the contours of the image\n",
    "    contours, hierarchies = cv.findContours(\n",
    "        uintimage, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate each of the centers of the image\n",
    "    for contour in contours:\n",
    "        M = cv.moments(contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            sites.append([cX, cY])\n",
    "            \n",
    "    return sites\n",
    "\n",
    "def transform_img(img, permute = (1,2,0)):\n",
    "    \n",
    "    img = img.permute(*permute)\n",
    "    return img\n",
    "\n",
    "def show_image(img, title = \"No title\", alpha = 1):\n",
    "    img = transform_img(img)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='gray', alpha = alpha)\n",
    "    plt.title(title)\n",
    "\n",
    "def extract_sites_coordinates(segmentation, index):\n",
    "    return list(zip(*np.where(segmentation[index][0] == 1)))\n",
    "\n",
    "def normalise(x):\n",
    "    min_val = torch.min(x)\n",
    "    max_val = torch.max(x)\n",
    "\n",
    "    # normalize the tensor\n",
    "    return (x - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoronoiZSlice():\n",
    "    path = './segmentation_and_background_composite.tif'\n",
    "    \n",
    "     \n",
    "    def __init__(self, img_size = (128,128), time_step=[0], l = 850, r = 6500, t = 0):\n",
    "        self.object = skimage.io.imread(self.path)\n",
    "        self.time_step = time_step\n",
    "        \n",
    "        self.l = l\n",
    "        self.r = r\n",
    "        self.t = t\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.images_horizontally = (r - l) // img_size[1]\n",
    "        self.subimages_in_image = self.images_horizontally * (2560 // img_size[0])\n",
    "        \n",
    "        print(f\"Loading object with shape: {self.object.shape}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.time_step) * self.subimages_in_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        specific_image = self.time_step[idx // self.subimages_in_image]\n",
    "        idx_in_img = idx % self.subimages_in_image\n",
    "\n",
    "        horiz = self.l + (self.img_size[0] * ((idx_in_img) % self.images_horizontally))\n",
    "        vert = self.t + (self.img_size[1] * floor((idx_in_img) / self.images_horizontally))\n",
    "        \n",
    "        segmentation = self.object[specific_image][0][vert: vert + self.img_size[1], horiz: horiz + self.img_size[0]]\n",
    "        cellular_image = self.object[specific_image][1][vert: vert + self.img_size[1], horiz: horiz + self.img_size[0]]\n",
    "        \n",
    "        segmentation_voronoi_sites = compute_voronoi_sites(segmentation)\n",
    "        segmentation_voronoi = torch.zeros(1,*self.img_size)\n",
    "        \n",
    "        for site in segmentation_voronoi_sites:\n",
    "            segmentation_voronoi[0, site[1], site[0]] = 1\n",
    "\n",
    "        return segmentation_voronoi, gaussian_filter(transforms.ToTensor()(segmentation), sigma=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14107cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = DataLoader(VoronoiZSlice(time_step = [0], l = 650, r = 6500), batch_size = 32, shuffle=True)\n",
    "val = DataLoader(VoronoiZSlice(time_step = [195],  l = 650, r = 6500), batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4\n",
    "img, tar = next(iter(train))\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "for i in range(n_samples):\n",
    "    a, b = img[i], tar[i]\n",
    "\n",
    "    plt.subplot(2, n_samples, 1 + i)\n",
    "    show_image(b, \"Segmentation Map\")\n",
    "    \n",
    "    plt.subplot(2, n_samples, 1 + n_samples + i)\n",
    "    show_image(a, \"Voronoi Sites\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels = 1):\n",
    "        # Input Size: 1 x 128 x 128\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d1 = nn.Conv2d(channels, 64, 4, 2, 1);\n",
    "        # Size: 64 x 64 x 64\n",
    "        \n",
    "        self.d2 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        # Size: 128 x 32 x 32\n",
    "        \n",
    "        self.d3 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        # Size: 256 x 16 x 16\n",
    "        \n",
    "        self.d4 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        # Size: 512 x 8 x 8\n",
    "        \n",
    "        self.d5 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        # Size: 512 x 4 x 4\n",
    "        \n",
    "        self.d6 = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "        )\n",
    "        \n",
    "        # Size: 512 x 2 x 2\n",
    "        \n",
    "        self.u1 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=4, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        # Size: 512 x 4 x 4\n",
    "        \n",
    "        self.u2 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512 * 2, 512, kernel_size=4, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Size: 512 x 8 x 8\n",
    "        self.u3 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ConvTranspose2d(512 * 2, 256, kernel_size=4, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        # Size: 256 x 16 x 16\n",
    "        self.u4 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ConvTranspose2d(256 * 2, 128, kernel_size=4, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        # Size: 128 x 32 x 32\n",
    "        \n",
    "        self.u5 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ConvTranspose2d(128 * 2, 64, kernel_size=4, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        # Size: 64 x 64 x 64\n",
    "        \n",
    "        self.u6 = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64 * 2, 1, kernel_size=4, stride = 2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Size: 1 x 128 x 128\n",
    "    def forward(self, x):\n",
    "        d1 = self.d1(x)\n",
    "        d2 = self.d2(d1)\n",
    "        d3 = self.d3(d2)\n",
    "        d4 = self.d4(d3)\n",
    "        d5 = self.d5(d4)\n",
    "        \n",
    "        latent_space = self.d6(d5)\n",
    "        \n",
    "        u1 = torch.cat((self.u1(latent_space), d5), dim=1)\n",
    "        u2 = torch.cat((self.u2(u1), d4), dim=1)\n",
    "        u3 = torch.cat((self.u3(u2), d3), dim=1)\n",
    "        u4 = torch.cat((self.u4(u3), d2), dim=1)\n",
    "        u5 = torch.cat((self.u5(u4), d1), dim=1)\n",
    "    \n",
    "        return self.u6(u5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels = 1):\n",
    "        super().__init__()\n",
    "        # Size: 2 x 128 x 128 (two images are stacked on each other)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_channels * 2, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Size: 64 x 64 x 64\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # Size: 128 x 32 x 32\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # Size: 256 x 16 x 16\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # Size: 512 x 8 x 8\n",
    "            nn.Conv2d(512, 1, 4, 2, 1, bias=False),      \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, imgA, imgB):\n",
    "        img = torch.cat((imgA, imgB), dim=1)\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a648b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "L_gan = nn.BCELoss()\n",
    "L_l1 = nn.L1Loss()\n",
    "lambda_pixel = 1000\n",
    "epochs = 300\n",
    "\n",
    "generator = UNet()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "Tensor = torch.FloatTensor\n",
    "\n",
    "generator.to(torch.device(\"mps\"))\n",
    "discriminator.to(torch.device(\"mps\"))\n",
    "L_gan.to(torch.device(\"mps\"))\n",
    "L_l1.to(torch.device(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8557af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "print(f\"Starting training at {datetime.datetime.now()}\")\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for img, tar in train:\n",
    "        img, tar = img.to(torch.device(\"mps\")), tar.to(torch.device(\"mps\"))\n",
    "\n",
    "        valid = torch.ones(size = (img.size(0),1,4,4), dtype=torch.float).to(torch.device(\"mps\"))\n",
    "        fake = torch.zeros(size = (img.size(0),1,4,4), dtype=torch.float).to(torch.device(\"mps\"))\n",
    "        \n",
    "        optim_D.zero_grad()\n",
    "        \n",
    "        # Check that the discriminator is able to predict real images with high probability\n",
    "        Dx = discriminator(img, tar)\n",
    "        lossD_real = 0.5 * L_gan(Dx, valid)\n",
    "        lossD_real.backward()\n",
    "        \n",
    "        # Instead of training the discriminator on generator images, we can try and train the discriminator on more \n",
    "        # 'realistic' fake images. Idea from Bretschneider paper\n",
    "        Gz = generator(img)\n",
    "        DGz = discriminator(img, Gz)\n",
    "        lossD_fake = 0.5 * L_gan(DGz, fake)\n",
    "        lossD_fake.backward()  \n",
    "        optim_D.step()\n",
    "        \n",
    "        # ----\n",
    "        # Train Generator\n",
    "        # ----\n",
    "        \n",
    "        optim_G.zero_grad()\n",
    "        Gz = generator(img)\n",
    "        outputs = discriminator(img, Gz) \n",
    "        loss_G = L_gan(outputs, valid) + lambda_pixel * L_l1(Gz, tar)\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            img, tar = next(iter(val))\n",
    "            img, tar = img.to(torch.device(\"mps\")), tar.to(torch.device(\"mps\"))\n",
    "\n",
    "            Gz = generator(img)\n",
    "\n",
    "            img_sample = torch.cat((img, tar, Gz), -2)\n",
    "            save_image(img_sample, f\"voronoi_map_to_image_no-deblur_{epoch}.png\", nrow=4)\n",
    "        \n",
    "    print(f\"Epoch {epoch}/{epochs} Generator Loss: {loss_G.item():.4f} Real Loss: {lossD_real.item():.2f} Fake Loss: {lossD_fake.item():.2f} Time: {datetime.datetime.now()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0668204",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'unet-128-pointcloud-to-segmentation-lambda-1000-fulladversarial.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387fd543",
   "metadata": {},
   "source": [
    "### Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2358a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = UNet()\n",
    "generator.load_state_dict(torch.load('/Users/siddharthsrivastava/Library/CloudStorage/OneDrive-UniversityofWarwick/Dissertation Models - Siddharth Srivastava/unet_voronoi_to_segmentation_128.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1067ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from skimage import filters\n",
    "\n",
    "import kornia\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites, tar, cell = next(iter(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = transforms.GaussianBlur(kernel_size = (11,11))(tar)\n",
    "\n",
    "plt.imshow(s[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdcfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imsave(\"/Users/siddharthsrivastava/Library/CloudStorage/OneDrive-UniversityofWarwick/Dissertation Models - Siddharth Srivastava/dataset/gan-segmentation-map_size-128-128_timestep-195.tiff\", Gz1.numpy(), plugin='tifffile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gz1 = generator(sites.to(device = 'mps')).detach().cpu()\n",
    "# Gz2 = generator(seg2).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cda15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vor = Voronoi(sites)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(1,2,1)\n",
    "fig = voronoi_plot_2d(vor)\n",
    "fig\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(seg[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d668a3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i + 1)\n",
    "    show_image(Gz1[i].cpu(), \"\")\n",
    "    # show_image(tar[i].cpu(), \"\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try adding a local median filter to remove the dumb black spots in the image\n",
    "med_filtered = normalise(torch.from_numpy(median_filter(Gz1, size=(1,1,3,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "tifffile.imsave(\"128_128_finaltimestep_ground_truth_seg_map.tiff\", tar.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ab69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply local contrast enhancement with CLAHE\n",
    "clahe = med_filtered.clone()\n",
    "for i in range(clahe.shape[0]):\n",
    "    clahe[i] = torch.from_numpy(equalize_adapthist(med_filtered[i][0].numpy(), kernel_size = 8)).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try some filtering\n",
    "thresholded_after_med_filter = torch.where(clahe >= filters.threshold_otsu(clahe.numpy()), 1, 0)\n",
    "\n",
    "# Add some dilation and thinning\n",
    "dilate = kornia.morphology.dilation(thresholded_after_med_filter, torch.ones(3,3))\n",
    "thinned = dilate.clone()\n",
    "for i in range(dilate.shape[0]):\n",
    "    thinned[i] = torch.from_numpy(skimage.morphology.skeletonize(dilate[i].squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i + 1)\n",
    "    show_image(med_filtered[i], \"\")\n",
    "    show_image(seg[i], \"\", alpha=0.7)\n",
    "    \n",
    "# plt.figure(figsize=(10,7))\n",
    "# for i in range(6):\n",
    "#     plt.subplot(2,3,i + 1)\n",
    "#     show_image(seg[i], \"\")\n",
    "#     show_image(tar[i], \"\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(seg[0].permute(2,1,0), cmap='gray')\n",
    "plt.imshow(thinned[0].permute(2,1,0), alpha=0.4, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_2_gan = UNet()\n",
    "stage_2_gan.load_state_dict(torch.load('../map_to_cell_generator_300.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe4240",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kornia.metrics.ssim(thinned, thinned2, 11).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5248bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1)\n",
    "show_image(dilated[0], \"Dilated Image\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "show_image(Gz1[0], \"Binary Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aff2b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(thinned_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = normalise(stage_2_gan(thinned).detach())\n",
    "res2 = normalise(stage_2_gan(thinned2).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de403c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "kornia.metrics.ssim(res, cell, 11).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ec9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i + 1)\n",
    "    show_image(res[i], \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i + 1)\n",
    "    show_image(seg[i], \"\")\n",
    "    show_image(res[i], \"\", alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a063fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
